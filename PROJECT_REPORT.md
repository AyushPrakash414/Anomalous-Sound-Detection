# Project Report: ImageToSpectrogramPrediction

This file is a human-readable, structured report describing the repository: files/folders, architecture, core functions and classes, dependencies, data/control flow, and suggestions to improve the project.

---

## 1) Project overview

This project converts audio into spectrogram images, trains/extracts features from an autoencoder, computes Mahalanobis-style anomaly scores in a reduced PCA latent space, and exposes a simple Flask-based web UI for inference.

Primary components:
- `AudioToSpectrogram.py` — audio → spectrogram converter script
- `project/` — model, preprocessing and server code (Flask app, feature extraction, parameter preparation)
- `gearbox/` and `SpectrogramImage/` — dataset folders (audio and generated images)
- Pretrained model & binary parameter files (`spectrogram_autoencoder_section00.h5`, `*.npy`)
- Notebooks for training/experimentation (`train.ipynb`, `traditional_train.ipynb`, `test.ipynb`)

---

## 2) Files & folders (what each contains)

- `AudioToSpectrogram.py` — Script to convert audio files (in `gearbox/train/`) into mel-spectrogram images saved as PNGs in `SpectrogramImage/train/`. Uses `librosa`, `numpy`, and `opencv`.

- `README.md` — Project README with usage notes and tips.

- `spectrogram_autoencoder_section00.h5` — Keras/TensorFlow saved model (pretrained autoencoder).

- `train.ipynb`, `traditional_train.ipynb`, `test.ipynb` — Jupyter notebooks for training and testing workflows.

- `gearbox/`
  - `train/` — training audio files (raw audio inputs expected by `AudioToSpectrogram.py`).
  - `source_test/`, `target_test/` — test audio folders (organized for experiments).

- `SpectrogramImage/`
  - `train/` — generated spectrogram images used for training/feature extraction.
  - `source_test/`, `target_test/` — image test folders.

- `project/` — the inference & supporting scripts and saved parameters:
  - `app.py` — Flask web application providing a UI and API (`/` and `/analyze`) that accepts audio or image files, computes a Mahalanobis score and returns a 3-level label.
  - `extract_features.py` — script to load the autoencoder encoder, run it over `SpectrogramImage/train/` and save `train_normal_features.npy` (latent vectors for normal training set).
  - `prepare_model_params.py` — loads `train_normal_features.npy`, runs PCA (n_components=128), computes mean vector and inverse covariance for Mahalanobis scoring, and saves PCA and Mahalanobis parameter `.npy` files (e.g., `pca_components.npy`, `mean_vec.npy`, `inv_cov.npy`).
  - `train_normal_features.npy` — saved latent features (generated by `extract_features.py`).
  - `pca_*.npy`, `mean_vec.npy`, `inv_cov.npy` — saved PCA parameters and Mahalanobis params used at inference time.
  - `static/` — static folder served by Flask (contains `uploads/` where uploaded files are saved).
  - `templates/` — HTML templates for the Flask app (contains `index.html`).

- `.gitignore`, `.git/` — repo metadata.

---

## 3) Architecture and module interaction

High-level architecture:
- Data preparation: raw audio in `gearbox/` → `AudioToSpectrogram.py` creates spectrogram PNG images under `SpectrogramImage/`.
- Model training (notebooks): use spectrogram images to train a convolutional autoencoder (not included as a single script here — notebooks contain training logic). The autoencoder is saved as `spectrogram_autoencoder_section00.h5`.
- Feature extraction: `project/extract_features.py` loads the autoencoder, extracts the encoder submodel, runs normal training images through it, and saves latent vectors (`train_normal_features.npy`).
- Parameter preparation: `project/prepare_model_params.py` runs PCA on latent vectors and computes mean vector and inverse covariance for Mahalanobis scoring, saving parameter `.npy` files.
- Inference & UI: `project/app.py` loads the saved model, PCA parameters and Mahalanobis params. It exposes a Flask UI and `/analyze` endpoint. The endpoint accepts audio or image files, converts audio to spectrogram (inside `process_audio`), extracts encoder features, reduces with PCA, computes a Mahalanobis distance and then returns a severity label (`NORMAL`, `NEEDS MAINTENANCE`, `SEVERE ANOMALY`).

Data flow diagram (simplified):

Raw audio → (AudioToSpectrogram.py) → Spectrogram image → Encoder (autoencoder) → Latent vector → PCA → Mahalanobis → Score → classify_3levels → UI/JSON result

Model generation flow:
Spectrogram images (normal) → encoder → collect latent vectors → PCA fit → compute mean & inv_cov → save params

---

## 4) Core functions & scripts (what they do, inputs, outputs, usage)

Below are the main functions and scripts in the repo with details.

- `AudioToSpectrogram.py`
  - `audio_to_mel_spectrogram(audio_path, save_path)`
    - Purpose: Convert an audio file to a mel-spectrogram, convert to dB, normalize to 0–255 and save as a single-channel PNG.
    - Inputs: `audio_path` (str path to audio file), `save_path` (str path where PNG will be written).
    - Outputs: Writes a PNG file to `save_path` (no return value).
    - Usage: Called for each audio file in `INPUT_FOLDER` (default points to `gearbox/train`), used to build `SpectrogramImage/train/` dataset.

  - Global parameters: `SAMPLE_RATE=22050`, `N_MELS=128`, `FMAX=8000` — adjust spectrogram resolution.

- `project/app.py` (Flask app)
  - `score_image_mahalanobis(img, encoder, pca, mean_vec, inv_cov)`
    - Purpose: Given a grayscale spectrogram image, run it through the encoder to get a latent vector, apply PCA transform, compute Mahalanobis distance between the transformed vector and the reference mean using the inverse covariance.
    - Inputs: `img` (2D numpy array grayscale spectrogram; expected to be scaled to 0..255), `encoder` (Keras model returning latent vector), `pca` (sklearn PCA object with preloaded attributes), `mean_vec` (1D numpy array), `inv_cov` (inverse covariance matrix).
    - Outputs: `float` — Mahalanobis distance (anomaly score).
    - Usage: Called by `process_audio()` and `process_image()`.

  - `process_audio(file_path)`
    - Purpose: load audio with `librosa`, compute mel-spectrogram, convert to dB, resize to model input size (224×224), and call `score_image_mahalanobis` to compute a score.
    - Inputs: `file_path` (path to uploaded audio file).
    - Outputs: anomaly score (float).
    - Usage: Used in Flask upload handling when user selects `audio` as input type.

  - `process_image(file_path)`
    - Purpose: read a grayscale spectrogram image from disk, resize to 224×224, and call `score_image_mahalanobis`.
    - Inputs: `file_path` (path to uploaded image file).
    - Outputs: anomaly score (float).
    - Usage: Used when users upload an image directly.

  - `classify_3levels(score)`
    - Purpose: map numeric score into one of three textual labels.
    - Logic: `score < 9 → NORMAL`, `9 ≤ score < 15 → NEEDS MAINTENANCE`, `score ≥ 15 → SEVERE ANOMALY`.
    - Inputs: `score` (float).
    - Outputs: label string.
    - Usage: used by Flask routes to display results.

  - Flask routes
    - `GET /` — renders `index.html` template.
    - `POST /` — receives file uploads from form, saves file to `static/uploads`, processes via `process_audio` or `process_image`, computes label and renders template with results.
    - `POST /analyze` — API endpoint used by AJAX in `index.html`, returns JSON with `result`, `score`, and `file_path`.

  - Model loading in `app.py`:
    - Loads `spectrogram_autoencoder_section00.h5`, then reconstructs an encoder by manually building inputs/connecting first several autoencoder layers and taking the encoder output layer (this depends on the exact layer order in the saved model). PCA and Mahalanobis parameters are loaded from `.npy` files.

- `project/extract_features.py`
  - Purpose: batch-extract latent vectors (encoder outputs) for all images in `TRAIN_NORMAL_DIR` and save them to `train_normal_features.npy`.
  - Inputs: images from `TRAIN_NORMAL_DIR` (default points to `SpectrogramImage/train`). The script expects images to be `.png` or `.jpg`.
  - Outputs: `train_normal_features.npy` — a 2D array of shape (n_samples, latent_dim).
  - Usage: run once after preparing normal spectrogram images; provides the data used for PCA fitting and Mahalanobis params.

- `project/prepare_model_params.py`
  - Purpose: load `train_normal_features.npy`, fit `sklearn.decomposition.PCA(n_components=128)`, compute `mean_vec` and `inv_cov` (inverse covariance of PCA-transformed features) and save all parameters to `.npy` files used at inference.
  - Inputs: `train_normal_features.npy` (generated by `extract_features.py`).
  - Outputs: saved parameter files: `pca_components.npy`, `pca_mean.npy`, `pca_explained_variance.npy`, `pca_explained_variance_ratio.npy`, `pca_singular_values.npy`, `mean_vec.npy`, `inv_cov.npy`.
  - Usage: run after feature extraction to prepare parameters used by `app.py`.

---

## 5) Key dependencies & why they are used

- `numpy` — numerical arrays and file-based parameter save/load (`np.save` / `np.load`).
- `librosa` — audio loading and mel-spectrogram computation; widely used and convenient functions for audio feature extraction.
- `opencv-python` (`cv2`) — image resizing, normalization and image saving (`cv2.imwrite`, `cv2.resize`). Convenient for image I/O in simple scripts.
- `tensorflow` / `keras` — model loading (`load_model`) and inference (autoencoder and encoder). Provides trained deep learning models and prediction APIs.
- `scikit-learn` (`sklearn.decomposition.PCA`) — PCA used to reduce encoder feature dimensionality and stabilize covariance estimation for Mahalanobis distance.
- `matplotlib` — included in `app.py` imports (not strictly required for inference; likely used by notebooks for visualization).
- `tqdm` — progress bar in `AudioToSpectrogram.py`.
- `flask` — web UI and API server for simple demo deployment.

Why these choices make sense:
- `librosa` is standard for audio preprocessing and offers mel spectrogram computation.
- `tensorflow/keras` is used for model consistency (autoencoder saved as `.h5`).
- PCA + Mahalanobis is a practical unsupervised anomaly detection approach on latent vectors.

---

## 6) High-level flow of data & control

1. Collect raw audio in `gearbox/train/`.
2. Run `AudioToSpectrogram.py` to convert audio files into mel-spectrogram PNGs saved under `SpectrogramImage/train/`.
3. Train an autoencoder (not included as a single script here — notebooks are used). Save the trained model as `spectrogram_autoencoder_section00.h5`.
4. Run `project/extract_features.py`: load saved autoencoder, extract encoder, compute latent vectors for each normal image; save `train_normal_features.npy`.
5. Run `project/prepare_model_params.py`: fit PCA on latent features, compute mean and inverse covariance for Mahalanobis scoring; save `.npy` parameters.
6. Start `project/app.py` (Flask): it loads the autoencoder, PCA params and Mahalanobis params. When an image or audio file is uploaded, the app computes a score and returns a human-readable label.

---

## 7) Suggestions for improvement (practical, prioritized)

1. Add `requirements.txt` with pinned versions (example: `tensorflow==2.x`, `librosa==0.8.x`, `opencv-python==4.x`, `numpy==1.23.*`) for reproducibility.
2. Add a small `predict.py` CLI wrapper that loads model/params and accepts a path to audio/image and prints JSON — useful for non-Flask usage and testing.
3. Improve model input robustness: centralize preprocessing (resize, normalization) into a single function used by all scripts to avoid mismatch between training and inference.
4. Validate uploaded files in `app.py` (file size limits, file extension checks, error handling) and ensure safe file paths (avoid path traversal). Add try/except around model prediction for graceful errors.
5. Replace manual encoder reconstruction (indexing layers) with named-layer extraction or save the encoder separately during training. Manual layer indexing can break if the model architecture changes.
6. Add unit tests for the preprocessing functions and a small end-to-end smoke test using a tiny synthetic audio file.
7. Add logging (instead of print statements) and configurable verbosity.
8. Consider using `scipy` or `librosa.util` to ensure consistent spectrogram resizing and interpolation; ensure consistent image orientation (time-frequency axes) across steps.
9. If productionizing, consider moving the inference API to FastAPI + Uvicorn and add async handling, model caching, and rate limiting.
10. Add a license file and `CONTRIBUTING.md`.
11. Include a small `samples/` folder with 1–2 example audio files and corresponding spectrogram images to make quick local tests easier.
12. Add platform notes for TensorFlow GPU support if intended for GPU training (CUDA/cudnn versions).

---

## 8) Short FAQ / quick actions

- How to generate spectrograms? Run:

```powershell
python AudioToSpectrogram.py
```

- How to extract features and prepare params? Run in order:

```powershell
python project/extract_features.py
python project/prepare_model_params.py
```

- How to run the web UI? In `project/` run:

```powershell
python app.py
# Then open http://127.0.0.1:5000/ in a browser
```

- What to check if scores are wrong?
  - Verify preprocessing consistency (resize, scaling) between training/extract_features and inference.
  - Ensure PCA params and Mahalanobis params are generated from the same encoder used at inference (matching architecture and weights).

---

## 9) Where I added this report

Report file: `PROJECT_REPORT.md` (this file)

---

If you want, I can now:
- add `requirements.txt` with pinned versions,
- add `predict.py` CLI wrapper and `README` usage examples,
- add a `Makefile` or simple PowerShell script to run the sequence (convert → extract → prepare → run),
- or run a quick smoke test using a short generated sine wave audio file to demonstrate end-to-end inference locally.

Which next step would you like me to do? 
